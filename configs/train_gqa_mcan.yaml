train_dataset: gqa
train_dataset_args:
  root_dir: ./assets/data/gqa
  split: train

test_dataset: gqa
test_dataset_args:
  root_dir: ./assets/data/gqa
  split: val

# model: mcan
# Note: Modifying the hyper-param of MCAN is not recommended. But you may find them in ./utils/__init__.py (MCAN_GQA_PARAMS)
model: mcan-customized
model_args:
  word_emb_path: ./cache/gqa_word_embed.npy
  encoder: transparent_superpixel_encoder
  encoder_args: {encoder: pvtv2_b2, use_boxes_dim: False}
load_encoder: ./cache/pvtv2_b2-{}.pth
encoder_pretrain: imagenet

train_batches: 1000000
ep_per_batch: 1
max_epoch: 12

# 0 -- original
# 1 -- systematic
eval_mode: 0

###########
# RelViT
###########
relvit: True
relvit_weight: 1.0
# temperature
relvit_loss_tau: 0.04
# 0 -- both
# 1 -- local only
# 2 -- global only
relvit_local_only: 0
# 0 -- EsViT
# 1 -- EsViT+RelViT
# 2 -- RelViT
relvit_mode: 1
# False -- Most recent
# True -- Uniform
relvit_sample_uniform: True
relvit_num_concepts: 1615

###########
# MoCo
###########
relvit_moco_m: 0.999
relvit_moco_use_queue: False
relvit_moco_K: 10
# 49 for pvtv2 and swin-s, 196 for vit-s-16
relvit_num_tokens: 49

optimizer: adamw
optimizer_args: {lr: 0.0001, weight_decay: 0, 'milestones': [8, 10], eps: 1e-8}

print_freq: 10
save_epoch: 1
eval_epoch: 1
grad_norm: 0.5